rvencu: then no problem, stay within 1-4 gpus range and you are fine
"also make sure all you scripts and data stay on /fsx this is the persistent storage
"

ssh username@login.hpc.stability.ai
# on a head node run screen
srun --comment harmonai --partition=gpu --gpus=1 --cpus-per-gpu=6 --nodes=1 --pty bash -i
srun --comment harmonai --partition=gpu --gpus=2 --cpus-per-gpu=6 --nodes=1 --pty bash -i


# Only do this once
python3.8 -m venv venv
source /fsx/turian/inverse-audio-synthesis/venv/bin/activate
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
pip3 install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu113

cd /fsx/turian/inverse-audio-synthesis/
source /fsx/turian/inverse-audio-synthesis/venv/bin/activate
export WANDB_CACHE_DIR=/fsx/turian/.cache

The next level up would be making a SLURM batch script, yeah

Please do not do data transfers from headnode. this traffic can
break slurm node checks and create havoc.

Consider:
CUDA_VISIBLE_DEVICES=1 


SGD training:
./train.py vicreg.optim.name=sgd vicreg.optim.args.lr=0.1
