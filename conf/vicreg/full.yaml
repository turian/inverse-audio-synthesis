limit_train_batches: null
# limit_train_batches: 100

# Set this as large as possible
#batch_size: 4
batch_size: 1024
# For precision 16
#batch_size: 2048

# Each checkpoint is around 1GB so
# too frequent it wandb will be costly.
checkpoint_every_nbatches: 100000
#checkpoint_every_nbatches: 1000
#checkpoint_every_nbatches: 10

val_check_interval: 1024
limit_val_batches: 128

mlp: "8192-8192-%d"

sim_coeff: 25.0
std_coeff: 25.0
cov_coeff: 1.0

pretrained_vision_model: True

optim:
  name: "sgd"
  args:
   lr: 0.1
#  name: "lars"
#  args:
#    base_lr: 2.0
#    weight_decay: 1e-6
#    # In LARS we use base_lr instead
#    lr: null
