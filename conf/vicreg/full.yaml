limit_train_batches: null
# limit_train_batches: 100

# Set this as large as possible
#batch_size: 4
#batch_size: 1024
batch_size: 128
# For precision 16
#batch_size: 2048

# Each checkpoint is around 1GB so
# too frequent it wandb will be costly.
checkpoint_every_nbatches: 10000
#checkpoint_every_nbatches: 1000
#checkpoint_every_nbatches: 10

# Otherwise this will be the biggest module in vicreg
mlp: "256-256-%d"
#mlp: "1024-1024-%d"

sim_coeff: 25.0
std_coeff: 25.0
cov_coeff: 1.0

pretrained_vision_model: True

optim:
  name: "sgd"
  args:
   lr: 0.1
#  name: "lars"
#  args:
#    base_lr: 2.0
#    weight_decay: 1e-6
#    # In LARS we use base_lr instead
#    lr: null
