limit_train_batches: null
# limit_train_batches: 100

# Set this as large as possible
#batch_size: 4
#batch_size: 1024
batch_size: 128
#batch_size: 64
# For precision 16
#batch_size: 2048

# Each checkpoint is around 1GB so
# too frequent it wandb will be costly.
# FYI: This batch number isn't steps :\
# It's some weird artifical number
checkpoint_every_nbatches: 1000000
#checkpoint_every_nbatches: 10000
#checkpoint_every_nbatches: 10

val_check_interval: 1024
limit_val_batches: 128

mlp: "8192-8192-%d"

sim_coeff: 25.0
std_coeff: 25.0
cov_coeff: 1.0

pretrained_vision_model: True

optim:
#  name: "sgd"
#  args:
#   lr: 0.032
##   lr: 0.01
  name: "lars"
  args:
    base_lr: 3.2    # 2.0 best with no lr schedule
    weight_decay: 1e-6
    # In LARS we use base_lr instead
    lr: null
scheduler:
  name: "LinearWarmupCosineAnnealingLR"
  args:
    # Try 10k later?
    warmup_epochs: 1000
    # ImageNet has 1281167 images and our batch size is 1024,
    # so our "epoch" is ~1251 steps. (+ warmup_epochs)
    max_epochs: 22510
    # Learning rate to start the linear warmup.
    warmup_start_lr: 0.0
    # Minimum learning rate.
    eta_min: 0.0
