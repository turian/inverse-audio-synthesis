#precision: 16
# Try this to avoid NaN too late through
#precision: bf16
accelerator: "gpu"
#devices: 1
devices: 2
#accelerator: "cpu"
#devices: 1
strategy: "ddp"

num_batches: 50000000

num_workers: 1

log: wand
#log: null

#dim: 512
dim: 256

# Number of torchsynth voice params
nparams: 78

seed: 42

ntest_batches: 1

torchsynth:
    reproducible: False
    buffer_size_seconds: 4.0
    # torchsynth default
    rate: 44100

vicreg:
    do_pretrain: True
    # do_pretrain: False

    limit_train_batches: null
    # limit_train_batches: 100

    # Set this as large as possible
    batch_size: 1024
    # For precision 16
    #    batch_size: 2048

#    checkpoint_every_nbatches: 10000
    checkpoint_every_nbatches: 1000
    #checkpoint_every_nbatches: 10

    # Otherwise this will be the biggest module in vicreg
    mlp: "256-256-%d"
    #    mlp: "1024-1024-%d"

    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0

    pretrained_vision_model: True

    optim:
#      name: "sgd"
#      args:
#        lr: 0.1
      name: "lars"
      args:
        base_lr: 2.0
        weight_decay: 1e-6

param_embed:
    # Used in heareval
    hidden_norm: nn.BatchNorm1d
    #hidden_norm: nn.Identity
    # Used in heareval
    dropout: 0.1


audio_to_params:
    batch_size: 1024

    # Used in heareval
    hidden_norm: nn.BatchNorm1d
    #hidden_norm: nn.Identity
    # Used in heareval
    dropout: 0.1

    checkpoint_every_nbatches: 1000
    #checkpoint_every_nbatches: 10

    limit_train_batches: null
    # limit_train_batches: 100

    optim:
#      name: "sgd"
#      args:
#        lr: 0.1
      name: "lars"
      args:
        base_lr: 2.0
        weight_decay: 1e-6

#mel:
#    n_fft: 1024
#    win_length: null
#    hop_length: 512
#    center: True
#    pad_mode: "reflect"
#    power: 2.0
#    norm: "slaney"
#    onesided: True
#    n_mels: 128
#    mel_scale: "htk"
