precision: 16
accelerator: "gpu"
devices: 1
#accelerator: "cpu"
#devices: 1

num_batches: 50000000

num_workers: 1

log: wand
#log: null

#dim: 512
dim: 256

# Number of torchsynth voice params
nparams: 78

seed: 42

torchsynth:
    reproducible: False
    buffer_size_seconds: 4.0
    # torchsynth default
    rate: 44100

vicreg:
    do_pretrain: True

    limit_train_batches: null
    # limit_train_batches: 100

    # Set this as large as possible
    batch_size: 512

    # Using LARS will require some gnarly pytorch DDP rewrite or pytorch lightning etc port
    use_lars: False
    #use_lars: True

#    checkpoint_every_nbatches: 10
    checkpoint_every_nbatches: 10000

    # Otherwise this will be the biggest module in vicreg
    mlp: "256-256-%d"
    #    mlp: "1024-1024-%d"

    sim_coeff: 25.0
    std_coeff: 25.0
    cov_coeff: 1.0
    weight_decy: 1e-6

    pretrained_vision_model: True

    lr: 0.1

param_embed:
    # Used in heareval
    hidden_norm: nn.BatchNorm1d
    #hidden_norm: nn.Identity
    # Used in heareval
    dropout: 0.1


audio_repr_to_params:
    # Smaller for downstream???? IDK WHY
    batch_size: 4

mel:
    n_fft: 1024
    win_length: null
    hop_length: 512
    center: True
    pad_mode: "reflect"
    power: 2.0
    norm: "slaney"
    onesided: True
    n_mels: 128
    mel_scale: "htk"
